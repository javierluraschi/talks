---
title: "Updates on Spark, MLflow, and the broader ML ecosystem"
subtitle: "with Sparklyr"
author: "[Javier Luraschi](https://github.com/javierluraschi)"
date: "August 2018"
output:
  revealjs::revealjs_presentation:
    css: styles/reveal.css
    df_print: paged
    self_contained: true
    center: true
    fig_width: 6
    fig_height: 4
render: rmarkdown::render("slides/slides.Rmd", output_format = "revealjs::revealjs_presentation", output_file = "the-r-in-spark.html") ; browseURL(paste0("file:///", file.path(getwd(), "slides/the-r-in-spark.html")))
fig_width: 6 
fig_height: 4 
---

```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

## Introduction


## Usage

```{r echo=FALSE, message=FALSE}
library(sparklyr)                                    # Load sparklyr
library(dplyr)                                       # Load dplyr
library(DBI)                                         # Load DBI

dir.create("input")                                  # Create cars folder
write.csv(mtcars, "input/cars.csv")                  # Write data in R
```

<div class="sparklyr-code">
<div>
```{r}
spark_install()                                      # Install Apache Spark
sc <- spark_connect(master = "local")                # Connect to Spark cluster
```
</div>
<div class="fragment">
```{r}
cars_tbl <- spark_read_csv(sc, "cars", "input/")     # Read data in Spark

summarize(cars_tbl, n = n())                         # Count records with dplyr
dbGetQuery(sc, "SELECT count(*) FROM cars")          # Count records with DBI
```
</div>
<div class="fragment">
```{r}
ml_linear_regression(cars_tbl, mpg ~ wt + cyl)       # Perform linear regression

ml_pipeline(sc) %>%                                  # Define Spark pipeline
  ft_r_formula(mpg ~ wt + cyl) %>%                   # Add formula transformation
  ml_linear_regression()                             # Add model to pipeline
```
</div>
<div class="fragment">
```{r}
spark_context(sc) %>% invoke("version")              # Extend sparklyr with Scala
```
</div>
<div class="fragment">
```{r}
spark_apply(cars_tbl, nrow)                          # Extend sparklyr with R
```
</div>
<div class="fragment">
```{r}
stream_read_csv(sc, "input/") %>%                    # Define Spark stream
  filter(mpg > 30) %>%                               # Add dplyr transformation
  stream_write_json("output/")                       # Start processing stream
```
</div>
</div>

# MLflow

## Introduction

# Demo

## Mastering Spark with R

## Community

## Linux Foundation


