---
title: Running R at Scale
subtitle: "with Apache Arrow on Spark"
author: "Javier Luraschi"
date: "Spark Summit 2019"
output:
  revealjs::revealjs_presentation:
    df_print: paged
    self_contained: true
    css: styles/reveal.css
    theme: white
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Overview

- Intro to R
- R with Spark
- Intro to Arrow
- Arrow with R
- Arrow on Spark

# Intro to R

- R Language
- R Packages
- An R Package

## R Language

R is a programming language for statistical computing that is: **vectorized**, **columnar** and **flexible**.

![](images/s-algorithm-interface.png)

## R Packages

**CRAN** is R's package manager, like NPM or Maven. Thousands of packages available and usage growing every year.

![](images/r-daily-cran-downloads.png)

## An R package

One of many packages, **rayrender**: A ray tracer written in R using Rcpp.

<div class="flow-table">
<div class="flow-column flow-column-image">![](images/r-package-rayrender-tweet.png)![](images/r-package-rayrender-teapot.gif)</div>
<div class="flow-column">
```{r}
library(rayrender)

generate_ground() %>%
  add_object(sphere()) %>%
  render_scene()
```
![](images/r-package-rayrender-github.png)</div></div>

# R with Spark

- sparklyr 0.4
- sparklyr 0.5
- sparklyr 0.6
- sparklyr 0.7
- sparklyr 0.8
- sparklyr 0.9
- sparklyr 1.0

## sparklyr 0.4 - Initial Release

Support to **install**, **connect**, **analyze**, **model** and **extend** Spark.

```{r}
spark_install()                                      # Install Apache Spark
sc <- spark_connect(master = "local")                # Connect to Spark cluster
```

```{r}
cars <- spark_read_csv(sc, "cars", "mtcars/")        # Read data in Spark

dplyr::summarize(cars, n = n())                      # Count records with dplyr
DBI::dbGetQuery(sc, "SELECT count(*) FROM cars")     # Count records with DBI
```

```{r}
ml_linear_regression(cars, mpg ~ wt + cyl)           # Perform linear regression
```

```{r}
spark_context(sc) %>% invoke("version")              # Extend sparklyr with Scala
```

## sparklyr 0.5 - Connections

Support for Apache **Livy**,

```{r}
sc <- spark_connect(master = "http://livy-server")   # Connect through Apache Livy 
```

**Databricks** connections,

```{r}
sc <- spark_connect(method = "databricks")           # Connect to Databricks cluster
```

`dplyr` improvements, and certified with **Cloudera**.

## sparklyr 0.6 - Distributed R

**Distribute R** computations to execute arbitrary R code over each **partition** using your favorite **R packages**:

<div class="flow-table">
<div class="flow-column flow-column-image"><a href="https://twitter.com/javierluraschi/status/1120557693721354240" target="_blank">![](images/r-package-rayrender-spark-tweet.png)</a>![](images/r-package-rayrender-spark.gif)</div>
<div class="flow-column distributed-code">
```{r}
scene <- generate_ground() %>%
  add_object(sphere(z = -2)) %>%
  add_object(sphere(z = +2)) %>%
  add_object(sphere(x = -2))

camera <- sdf_len(sc, 628, repartition = 628) %>%
  mutate(x = 12 * sin(id/100), z = 12 * cos(id/100))

spark_apply(
  camera,
  function(cam, scene) {
    name <- sprintf("%04d.png", cam$id)
    rayrender::render_scene(
      scene, width = 1920, height = 1080,
      lookfrom = c(cam$x, 5, cam$z),
      filename = name)
    system2("hadoop", c("fs", "-put", name, "path"))
  }, context = scene) %>% collect()
```

    
</div></div>

## sparklyr 0.7 - Pipelines and Machine Learning

Provide a uniform set of high-level APIs to help create, tune, and **deploy** machine learning pipelines at **scale**,

```{r}
pipeline <- ml_pipeline(sc) %>%                      # Define Spark pipeline
  ft_r_formula(mpg ~ wt + cyl) %>%                   # Add formula transformation
  ml_linear_regression()                             # Add model to pipeline

fitted <- ml_fit(pipeline, cars)                     # Fit pipeline
```

and support for **all MLlib** algorithms.

## sparklyr 0.8 - MLeap and Graphs

MLeap allows you to use your Spark **pipelines** in any **Java** enabled device or service,

```{r}
library(mleap)                                       # Import MLeap package
install_maven()                                      # Install Maven
install_mleap()                                      # Install MLeap

transformed <- ml_transform(fitted, cars)            # Fit pipeline with dataset
ml_write_bundle(fitted, transformed, "model.zip")    # Export model with MLeap
```

graphframes provides an interface to the **GraphFrames** Spark package.

```{r}
library(graphframes)                                 # Import graphframes package

g <- gf_graphframe(edges = edges_tbl)                # Map to graphframe
gf_pagerank(g, tol = 0.01)                           # Compute pagerank
```

## sparklyr 0.9 - Streams

Spark **structured streams** provide parallel and fault-tolerant data processing,

```{r}
stream_read_text(sc, "s3a://your-s3-bucket/") %>%    # Define input stream
  spark_apply(~webreadr::read_s3(.x$line),) %>%      # Transform with R
  group_by(uri) %>%                                  # Group using dplyr
  summarize(n = n()) %>%                             # Count using dplyr
  arrange(desc(n)) %>%                               # Arrange using dplyr
  stream_write_memory("urls", mode = "complete")     # Define output stream
```

enables support for **Kubernetes** and to properly interrupt long-running operations.

```{r}
sc <- spark_connect(config = spark_config_kubernetes("k8s://hostname:8443"))
```

## sparklyr 1.0 - Arrow

- **Arrow** enables faster and larger data transfers between Spark and R.
- **XGBoost** enables training gradient boosting models over distributed datasets.

```{r}
library(sparkxgb)
dplyr::mutate(cars, eff = mpg > 20) %>%
  xgboost_classifier(eff ~ ., num_class = 2)
```

- **TFRecords** writes TensorFlow records from Spark to support deep learning workflows.

```{r}
library(sparktf)
cars %>% spark_write_tfrecord("tfrecord")
```

# Intro to Arrow

- What is Arrow?
- The Feather project
- The Arrow R package

## What is Arrow?

Apache Arrow is a **cross-language** development platform for **in-memory** data.

![](images/arrow-common-data-layer.png)

: Source: arrow.apache.org

## Memory Layout

**Columnar** memory layout allows applications to avoid **unnecessary IO** and accelerate analytical processing performance on modern CPUs and GPUs.

![](images/arrow-columnar-memory.png)

: Source: arrow.apache.org

# Arrow with R

- Feather package
- Arrow package

## Feather package

A **lightweight** binary columnar data **store** designed for maximum **speed**, based on Arrow's memory layout.

```{r}
library(feather)                                     # Import feather package

write_feather(mtcars, "cars.feather")                # Write feather file in R
read_feather("cars.feather")                         # Read feather file in R
```

```{python}
import feather                                       # Import feather package

df = feather.read_dataframe("cars.feather")          # Read feather file in Python
feather.write_dataframe(df, "cars.feather")          # Write feather file in Python
```

## Arrow package

Currently, install from GitHub:

```{r}
devtools::install_github("apache/arrow", subdir = "r", ref = "apache-arrow-0.13.0")
```

The R arrow package supports feather, parquet, streams, and more.

```{r}
library(arrow)                                       # Import arrow package

read_feather("cars.feather")                         # Can still read feather file
read_parquet("cars.parquet")                         # Can also read parquet files

write_arrow(mtcars, raw())                           # Can efficiently serialize
```
```
[1] 44 02 00 00 10 00 00 00 00 00 0a 00
```

# Arrow on Spark

## Requirements

To use Arrow with Spark and R you'll need:

- A **Spark 2.3.0**+ cluster.
- **Arrow 0.13**+ instealled in every node, Arrow 0.11+ usable.
- **R 3.5**+, next version is likely to support R 3.1+.
- **sparklyr 1.0**+.

## Implementation

R transformations in Spark **without** and **with Arrow**:

```{r echo=FALSE}
nomnoml::nomnoml("
#direction: right
#.left: align=left
[Spark|[a,1,10|b,2,20|c,3,30]] -> [Transform]
[Transform] -> [R]
[R] -> [Transform]
[Transform] -> [Spark]
[R|[a,1,10|b,2,20|c,3,30] -> [Transform]
[Transform] -> [<left>a,b,c|1,2,3|10,20,30]
[a,b,c] -> [Transform]
[Transform] -> [a,1,10]]", png = "images/transform-architecture.png")
```
```{r echo=FALSE}
nomnoml::nomnoml("
#direction: right
#.left: align=left
[Spark|[Scala|[a,1,10|b,2,20|c,3,30]] -> [Arrow|[<left>a,b,c|1,2,3|10,20,30]]
[Arrow] -> [Scala]
] -> [R]
[R|[Arrow|[<left>a,b,c|1,2,3|10,20,30]]] -> [Spark]", png = "images/transform-architecture-arrow.png")
```

![](images/transform-architecture.png)![](images/transform-architecture-arrow.png)

## Copy with Arrow

Copy **10x larger** datasets and **3x faster** with Arrow and Spark.

```{r}
library(arrow)
copy_to(sc, data.frame(y = 1:10^6))
```

![](images/spark-r-arrow-copy-to.png)

## Collect with Arrow

Collect **5x larger** datasets and **3x faster** with Arrow and Spark.

```{r}
library(arrow)
sdf_len(sc, 10^7) %>% collect()
```

![](images/spark-r-arrow-collect.png)

## Transform with Arrow

Transform datasets **40x faster** with R, Arrow and Spark.

```{r}
library(arrow)
sdf_len(sc, 10^5) %>% spark_apply(~.x/2) %>% count()
```

![](images/spark-r-arrow-transform.png)

# Thank you!

## Resources

- **Docs**: spark.rstudio.com
- **GitHub**: github.com/rstudio/sparklyr
- **Blog**: blog.rstudio.com/tags/sparklyr
- **R Help**: community.rstudio.com
- **Spark Help**: stackoverflow.com/tags/sparklyr
- **Issues**: github.com/rstudio/sparklyr/issues
- **Chat**: gitter.im/rstudio.sparklyr
- **Twitter**: twitter.com/hashtag/sparkly
