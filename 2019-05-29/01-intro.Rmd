---
title: "Introduction to Deep Learning -- Intro"
output: html_notebook
---

# Intro

## 1958: Rosenblatt -- Perceptron (OR)

```{r}
tt_or <- data.frame(
  a = c(0, 0, 1, 1),
  b = c(0, 1, 0, 1),
  r = c(0, 1, 1, 1)
)
```

```{r}
x <- as.matrix(select(tt_or, a, b))

b <- __
w <- c(__, __)

ifelse(x %*% w + b > 0, 1, 0)
```

## 1958: Rosenblatt -- Perceptron (AND)

```{r}
tt_and <- data.frame(
  a = c(0, 0, 1, 1),
  b = c(0, 1, 0, 1),
  r = c(0, 0, 0, 1)
)
```

```{r}
x <- as.matrix(select(tt_and, a, b))

b <- __
w <- c(__, __)

ifelse(x %*% w + b > 0, 1, 0)
```

## 1969: Minsky and Papert -- Perceptron (XOR)

```{r}
tt_xor <- data.frame(
  a = c(0, 0, 1, 1),
  b = c(0, 1, 0, 1),
  r = c(0, 1, 1, 0)
)
```

```{r}
x <- as.matrix(select(tt_xor, a, b))

b <- __
w <- c(__, __)

ifelse(w %*% x + b > 0, 1, 0)
```

## 1969: Minsky and Papert -- Layered (XOR)

```{r}
tt_xor <- data.frame(
  a = c(0, 0, 1, 1),
  b = c(0, 1, 0, 1),
  c = c(1, 1, 1, 1),
  r = c(0, 1, 1, 0)
)
```

```{r}
x <- as.matrix(select(tt_xor, a, b, c))
w <- matrix(c( ___,  ___,  ___,
               ___,  ___,  ___,
               ___,  ___,  ___), ncol = 3, byrow = T)
              
yh1 <- ifelse(x %*% w[1,] > 0, 1, 0)
yh2 <- ifelse(x %*% w[2,] > 0, 1, 0)

x3 <- matrix(c(yh1, yh2, c(1,1,1,1)), ncol = 3)
ifelse(x3 %*% w[3,] > 0, 1, 0)
```

## 1985: Hinton -- Exercise (AND)

Initialize,

```{r}
w_1=0.1; w_2=0.2; b=0.3
step=0.01;

f <- function(w_1, w_2, b, x_1, x_2) w_1 * x_1 + w_2 * x_2 + b
```

Approximate,

```{r}
for (i in 1:100) {
  f_1 <- f(w_1, w_2, b, tt_and$a, tt_and$b)
  w_1 <- w_1 - ___
  w_2 <- w_2 - ___
  b <- b - ___
}
```

## 2016: Karpathy -- Exercise (Graph)

Define the computation graph for the sigmoid function.

$$ \sigma (x) =  \frac{1 }{1 + e^{-x} } $$

```{r}
graph <- list(
  forward = function(x) -1 * x,
  backward = function(x) -1,
  node = list(
    forward = ___,
    backward = ___,
    node = ___
  )
)
```