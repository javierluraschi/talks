---
title: "Introduction to Deep Learning"
author: "Kevin Kuo, Javier Luraschi"
output:
  revealjs::revealjs_presentation:
    df_print: paged
    self_contained: true
    theme: white
    css: styles/reveal.css
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = FALSE)
library(nomnoml)
```

# Welcome

## Intros

- Kevin Kuo @kevinykuo
- Javier Luraschi @javierluraschi

## Agenda

- Welcome!
- Introduction to Deep Learning
- Introduction to Tensorflow
- Introduction to Keras

## Local Installation

![](images/welcome-installation-email.png)

## Workshop Notebook

[github.com/javierluraschi/deeplearning-sdss-2019](https://github.com/javierluraschi/deeplearning-sdss-2019)

![](images/welcome-notebook-repo.png)

## Why Deep Learning?

Why should I care about Deep Learning?

![](images/welcome-deeplearning-why.jpg)

## ImageNet

2012: [ImageNet classification with deep convolutional neural networks]()

![](images/welcome-imagenet-yanofsky.png)

## AlphaGo

2016: AlphaGo wins match against Lee Sedol ranked 9-dan, one of the best players at Go.

![](images/welcome-alphago.jpg)

## Popularizing Deep Learning

2017: HBO's Sillicon Valley show "hot dog or not" episode.

![](images/welcome-hotdog-hbo.png)

## Dota

2018: OpenAI Five defeats top 99.95th percentile Dota players

![](images/welcome-openai-dota.jpg)

## Tesla

2019: Tesla's autonomy day presents autonomous driving plan.

![](images/welcome-tesla-autonomous-day.png)

# Introduction to Deep Learning

## A Comprehensive Survey on Deep Learning Approaches

![](images/history-deeplearning.png)

See [arXiv:1803.01164](https://arxiv.org/abs/1803.01164)

## [1943: McCulloch and Pitts](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)

McCulloch & Pitts show that neurons can be combined to construct a Turing machine (using ANDs, ORs, & NOTs).

![](images/mcculloch-pitts-diagram.png)

## 1943: McCulloch and Pitts -- Turing Machines

![](images/mcculloch-pitts-turing-machine-example.jpg)

## 1958: Rosenblatt -- The Perceptron

[The perceptron: A probabilistic model for information storage and organization in the brain](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4E9CF4E9BFB2696E709F82AE0555531A?doi=10.1.1.588.3775&rep=rep1&type=pdf)

![](images/rosenblatt-perceptron-paper.png)

## 1958: Rosenblatt -- The Perceptron

### Perceptron Diagram

```{nomnoml eval=TRUE, echo=FALSE, width='600px', height='200px'}
#lineWidth: 1
#fontSize: 8
#padding: 10 
#.fun: visual=ellipse 
#spacing: 20
[x1]->[w1]
[x2]->[w2]
[xN]->[wN]
[w1]->[<fun> Σ]
[w2]->[Σ]
[wN]->[Σ]
[b]->[Σ]
[<sum> Σ]->[<fun>T]
[T]->[<hidden>Next]
```

### Perceptron Model

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

## 1958: Rosenblatt -- Perceptron (OR)

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 1, 1, 1)

b <- __
w <- c(__, __)

ifelse(x %*% w + b > 0, 1, 0) == r
```

## 1958: Rosenblatt -- Solution (OR)

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 1, 1, 1)

b <- 0
w <- c(0.5, 0.5)

ifelse(x %*% w + b > 0, 1, 0) == r
```

## 1958: Rosenblatt -- Perceptron (AND)

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 0, 0, 1)

b <- __
w <- c(__, __)

ifelse(x %*% w + b > 0, 1, 0) == r
```

## 1958: Rosenblatt -- Solution (AND)

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 0, 0, 1)

b <- -0.5
w <- c(0.5, 0.5)

ifelse(x %*% w + b > 0, 1, 0) == r
```

## 1958: Rosenblatt -- Demo

[Rosenblatt, with the image sensor of the Mark I Perceptron](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/) (...) it learned to differentiate between right and left after fifty attempts.

![](images/rosenblatt-perceptron-demo.jpg)

## 1958: Rosenblatt -- Predictions

[Expected to walk, talk, see, write, reproduce itself and be conscious of its existence, although (...) it learned to differentiate between right and left after fifty attempts.](https://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html)

![](images/rosenblatt-new-york-times.jpg)

## 1958: Rosenblatt -- [Principles of Neurodynamics 1/3](https://apps.dtic.mil/dtic/tr/fulltext/u2/256582.pdf)

![](images/rosenblatt-principles-neurodynamics.png)

## 1958: Rosenblatt -- [Principles of Neurodynamics 2/3](https://apps.dtic.mil/dtic/tr/fulltext/u2/256582.pdf)

![](images/rosenblatt-principles-neurodynamics-multilayer-diagram.png)

## 1958: Rosenblatt -- [Principles of Neurodynamics 3/3](https://apps.dtic.mil/dtic/tr/fulltext/u2/256582.pdf)

![](images/rosenblatt-principles-neurodynamics-unclassified.png)

## 1969: Minsky and Papert -- Book (1/3)

![](images/minsky-papert-perceptron.png)

## 1969: Minsky and Papert -- Book (2/3)

![](images/minsky-papert-perceptron-cover.png)

## 1969: Minsky and Papert -- Exercise (XOR)

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 1, 1, 0)

b <- __
w <- c(__, __)

ifelse(x %*% w + b > 0, 1, 0) == r
```

## 1969: Minsky and Papert -- Book (3/3)

[It ought to be possible to devise a training algorithm to optimize the weights in this using (...) we have not investigated this](https://www.quora.com/Why-did-Minsky-incorrectly-conjecture-the-inability-of-multi-layer-perceptrons-to-learn-non-linear-functions).

![](images/minsky-papert-multi-layer-perceptron.jpeg)

## 1969: Minsky and Papert -- Diagram (XOR)

Using multiple layered perceptrons should allow us to find a solution for the XOR table.

```{nomnoml eval=TRUE, echo=FALSE, width='800px', height='500px', fig.align='center'}
#lineWidth: 1
#fontSize: 8
#padding: 10 
#.fun: visual=ellipse 
#spacing: 20
[x]->[Perceptron(1)|
  
[x1]->[w1]
[x2]->[w2]
[xN]->[wN]
[w1]->[<fun> Σ]
[w2]->[Σ]
[wN]->[Σ]
[b]->[Σ]
[<sum> Σ]->[<fun>T]
        
]
[x]->[Perceptron(2) | 

[x1]->[w1]
[x2]->[w2]
[xN]->[wN]
[w1]->[<fun> Σ]
[w2]->[Σ]
[wN]->[Σ]
[b]->[Σ]
[<sum> Σ]->[<fun>T]

]
[Perceptron(1)]->[Perceptron(3) |

[b1]->[w1]
[b2]->[w2]
[w1]->[<fun> Σ]
[w2]->[Σ]
[b]->[Σ]
[<sum> Σ]->[<fun>T]
                    
]
[Perceptron(2)]->[Perceptron(3)]
[Perceptron(3)]->[<hidden>Next]
```

## 1969: Minsky and Papert -- Layered (XOR)

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1,
              1, 1, 1, 1), nrow = 4)
r <-        c(0, 1, 1, 0)

w <- matrix(c( ___,  ___,  ___,
               ___,  ___,  ___,
               ___,  ___,  ___), ncol = 3, byrow = T)
              
yh1 <- ifelse(x %*% w[1,] > 0, 1, 0)
yh2 <- ifelse(x %*% w[2,] > 0, 1, 0)

x3 <- matrix(c(yh1, yh2, c(1,1,1,1)), ncol = 3)
ifelse(x3 %*% w[3,] > 0, 1, 0) == r
```

## 1969: Minsky and Papert -- Solution (XOR)

```{r}
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1,
              1, 1, 1, 1), nrow = 4)
r <-        c(0, 1, 1, 0)

w <- matrix(c( 0.5,  0.5,    0,
              -0.5, -0.5,    1,
               0.5,  0.5, -0.5), ncol = 3, byrow = T)
              
yh1 <- ifelse(x %*% w[1,] > 0, 1, 0)
yh2 <- ifelse(x %*% w[2,] > 0, 1, 0)

x3 <- matrix(c(yh1, yh2, c(1,1,1,1)), ncol = 3)
ifelse(x3 %*% w[3,] > 0, 1, 0) == r
```

## 1985: Hinton -- Gradient Descent

[A Learning Algorithm for Boltzmann Machines](https://www.enterrasolutions.com/media/docs/2013/08/cogscibm.pdf)

![](images/hinton-1985-paper.png)

## 1985: Hinton -- [Gradient Descent Today](https://en.wikipedia.org/wiki/Gradient_descent)

A function decreases fastest if one goes from in the direction of the negative gradient.

$$
a_{n+1} = a_n - \gamma \nabla F(a_n)
$$

![](images/hinton-1985-gradient-descent.png)

## 1985: Hinton -- Gradient Descent

![](images/hinton-1985-gradient-descent-algo.png)

## 1985: Hinton -- Stochastic Gradient Descent

![](images/hinton-1985-stochastic-gradient-descent-algo.png)

## 1985: Hinton -- Back-Propagation

![](images/hinton-1985-back-propagation-algo.png)

## 1985: Hinton -- Exercise (AND) -- Differentiable

Gradient descent requires differentiable functions,

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

We use instead the ReLU, which is almost differentiable:

$$ max(0,\sum_{i=1}^m w_i x_i + b) $$

## 1985: Hinton -- Exercise (AND) -- Differentiate

Lets differentiate L2 over a perceptron with ReLU, where $\theta$ is the step function.

$$
(f(w_1, w_2, b) - y)^2 = (max(0,(w_1 x_1 + w_2 x_2 + b)) - y)^2
$$

$$
\frac{df(w_1, w_2, b)}{w_1} = 2 * (f(w_1, w_2, b) - y) * \theta({w_1 x_1 + w_2 x_2 + b}) * x_1 \\
\frac{df(w_1, w_2, b)}{w_2} = 2 * (f(w_1, w_2, b) - y) * \theta({w_1 x_1 + w_2 x_2 + b}) * x_2 \\
\frac{df(w_1, w_2, b)}{b} = 2 * (f(w_1, w_2, b) - y) * \theta({w_1 x_1 + w_2 x_2 + b}) 
$$
The we can iterate following the gradients direction,

$$
a_{n+1} = a_n - \gamma \nabla F(a_n)
$$

## 1985: Hinton -- Exercise (AND)

```{r}
w_1=0.1; w_2=0.2; b=0.3; learn=0.1;
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 0, 0, 1)

f <- function(w_1, w_2, b, x_1, x_2) w_1 * x_1 + w_2 * x_2 + b
step <- function(x) ifelse(x < 0, 0, 1)

for (i in 1:1000) {
  f_1 <- f(w_1, w_2, b, x[,1], x[,2])
  w_1 <- w_1 - ___
  w_2 <- w_2 - ___
  b <- b - ___
}

(f(w_1, w_2, b, x[,1], x[,2]) > 0.01) == as.logical(r)
```

## 1985: Hinton -- Solution (AND)

```{r}
w_1=0.1; w_2=0.2; b=0.3; learn=0.1;
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
r <-        c(0, 0, 0, 1)

f <- function(w_1, w_2, b, x_1, x_2) w_1 * x_1 + w_2 * x_2 + b
step <- function(x) ifelse(x < 0, 0, 1)

for (i in 1:1000) {
  f_1 <- f(w_1, w_2, b, x[,1], x[,2])
  w_1 <- w_1 - sum(learn * (2 * (pmax(0, f_1) - r) * step(f_1) * x[,1]))
  w_2 <- w_2 - sum(learn * (2 * (pmax(0, f_1) - r) * step(f_1) * x[,2]))
  b <- b - sum(learn * (2 * (pmax(0, f_1) - r)) * step(f_1))
}

(f(w_1, w_2, b, x[,1], x[,2]) > 0.01) == as.logical(r)
```

## 1985: Hinton -- Applications

1989: [ALVINN: An Autonomous Land Vehicle in a Neural Network](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf)

![](images/hinton-1985-applciations.png)

[https://www.youtube.com/watch?v=ntIczNQKfjQ](https://www.youtube.com/watch?v=ntIczNQKfjQ)

## 1985: Hinton -- Deep Networks

The [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) is when the gradient will be vanishingly small, effectively preventing the weight from changing its value.

```{nomnoml eval=TRUE, echo=FALSE, width='800px', height='500px', fig.align='center'}
#spacing: 30
#padding: 14
#bendSize: 0.7
#.p: visual=ellipse 
[<hidden>x1]->[<p>P1]
[<hidden>x2]->[<p>P2]
[<hidden>x3]->[<p>P3]
[P1]->[<p>P4]
[P2]->[P4]
[P3]->[P4]
[P1]->[<p>P5]
[P2]->[P5]
[P3]->[P5]
[P4]->[<p>P6]
[P5]->[P6]
[P4]->[<p>P7]
[P5]->[P7]
[P6]->[<p>P8]
[P7]->[P8]
[P6]->[<p>P9]
[P7]->[P9]
[P8]->[<p>P10]
[P9]->[P10]
[P10]->[<hidden>x]
```

## 2006: Hinton -- Train One Layer

[A fast learning algorithm for deep belief nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)

![](images/hinton-2006-train-one-layer.png)

## 2006: Hinton -- Autoencoders

[Reducing the dimensionality of data with neural networks](https://www.cs.toronto.edu/~hinton/science.pdf)

![](images/hinton-2006-autoencoder.png)

## 2006: Hinton -- Dimensionality

[Reducing the dimensionality of data with neural networks](https://www.cs.toronto.edu/~hinton/science.pdf)

![](images/hinton-2006-autoencoder-as-pca.png)

## 2012: AlexNet -- 

[ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

![](images/alexnet-2012-architecture.png)

Used ReLU, dropout, augmentation, GPUs.

## 2016: Karpathy -- Computational Graphs

Computational graphs avoid manually computing gradients.

![](images/Karpathy-2016-computation-graph.png)

[CS231n Winter 2016: Lecture 4: Backpropagation, Neural Network](https://www.youtube.com/watch?v=i94OvYb6noo)

## 2016: Karpathy -- Simple Graph

Simple computational graph example.

![](images/Karpathy-2016-simple-graph.png)

## 2016: Karpathy -- Complex Graph

Can then combine into very complex graphs using the chain rule:

![](images/Karpathy-2016-complex-graph.png)

## 2016: Karpathy -- Exercise (Graph)

Define the computation graph for the sigmoid function.

$$ \sigma (x) =  \frac{1 }{1 + e^{-x} } $$

```{r}
graph <- list(
  forward = function(x) -1 * x,
  backward = function(x) -1,
  node = list(
    forward = ___,
    backward = ___,
    node = ___
  )
)
```

## 2016: Karpathy -- Solution (Graph)

```{r}
graph <- list(
  forward = function(x) -1 * x,
  backward = function(x) -1,
  node = list(
    forward = function(x) e ^ (-x),
    backward = function(x) - e ^ (-x),
    node = list(
      forward = function(x) x + 1,
      backward = function(x) 1,
      node = list(
        forward = function(x) 1 / x,
        backward = function(x) - 1 / x ^ 2
      )
    )
  )
)
```

# Introduction to Tensorflow

## What is TensorFlow?

![](images/tensorflow-what-is.png)

[Slides from JJ's rstudio::cong 2018 Keynote](https://beta.rstudioconnect.com/ml-with-tensorflow-and-r)

## Why should R users care?

- A new general purpose numerical computing library!
  - Hardware independent
  - Distributed execution
  - Large datasets
  - Automatic differentiation
- Very general built-in optimization algorithms (SGD, Adam) that don't require that all data is in RAM
- Robust foundation for machine learning and deep learning applications
- TensorFlow models can be deployed with a low-latency C++ runtime
- R has a lot to offer as an interface language for TensorFlow

## What is tensor "flow"?

![](images/tensorflow-what-flows.png)

## Graph is generated from Code

![](images/tensorflow-code-generation.png)

## What is deep learning?

A simple mechanism that, once scaled, ends up looking like magic

![](images/tensorflow-what-is-deeplearning.png)

## Keras Adoption

![](images/tensorflow-keras-adoption.png)

## Keras for R cheatsheet

[github.com/rstudio/cheatsheets/raw/master/keras.pdf](https://github.com/rstudio/cheatsheets/raw/master/keras.pdf)

![](images/tensorflow-keras-cheatsheet.png)

# Introduction to Keras

## Installing Tensorflow -- Exercise 

```{r}
install.packages("tensorflow")
install_tensorflow()
```


## Installing Keras -- Exercise

```{r}
install.packages("keras")
install_keras()
```

## Kyphosis Dataset

We will use `Kyphosis` from the `rpart` package.

```{r}
download.file("https://github.com/bethatkinson/rpart/raw/master/data/kyphosis.tab.gz", "kyphosis.tab.gz")
kyphosis <- read.table("kyphosis.tab.gz", header = T)
```

```{r}
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
fit2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,
              parms = list(prior = c(0.65, 0.35), split = "information"))
fit3 <- rpart(Kyphosis ~ Age + Number + Start, data=kyphosis,
              control = rpart.control(cp = 0.05))

```

## Kyphosis Keras -- Exercise Model

Original analysis by [Derrick Mwiti](https://heartbeat.fritz.ai/binary-classification-using-keras-in-r-ef3d42202aaa).

```{r}
model <- keras_model_sequential() 

model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'sigmoid')
```

## Kyphosis Keras -- Exercise Compile

```{r}
history <- model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```

## Kyphosis Keras -- Exercise Train

```{r}
model %>% fit(
  X_train, y_train, 
  epochs = 100, 
  batch_size = 5,
  validation_split = 0.3
)
```

## Kyphosis Keras -- Exercise Summary

```{r}
summary(model)
```

## Kyphosis Keras -- Exercise Accuracy

```{r}
model %>% evaluate(X_test, y_test)

plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="orange", type="l")
lines(history$metrics$val_loss, col="skyblue")
legend("topright", c("Training","Testing"), col=c("orange", "skyblue"), lty=c(1,1))
```

## Kyphosis Keras -- Exercise Predict

```{r}
predictions <- model %>% predict_classes(X_test)
```

## JJ's Keynote

[beta.rstudioconnect.com/ml-with-tensorflow-and-r](https://beta.rstudioconnect.com/ml-with-tensorflow-and-r)

