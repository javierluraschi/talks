---
title: "Gradient Descent in R"
output: html_notebook
---

Let's start with a perceptron,

$$
p(x) =
\begin{cases}
1 & wx + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

Gradient descent minimizes a function by following the direction of maximum change:

$$
a_{n+1} = a_n - \gamma \nabla F(a_n)
$$

Let's define $f$ as,

$$ f(w, x, b) = wx + b $$

But we use a differentiable perceptron,

$$ p(x) = max(0, f(w, x, b)) $$

Now we need to minimize,

$$
l(w, x, b) = (p(w, x, b) - y)^2 = (max(0,f(w, x, b)) - y)^2
$$


And differentiate,

$$
\frac{dl(w, x, y)}{w_1} = 2 * (p(x) - y) * \theta(f(x)) * x_1 \\
\frac{dl(w, x, y)}{w_2} = 2 * (p(x) - y) * \theta(f(x)) * x_2 \\
\frac{dl(w, x, y)}{b} = 2 * (p(x) - y) * \theta(f(x)) 
$$

To train this single-layer single-perceptron,

```{r}
w <- c(0.1, 0.2); b=0.3; learn=0.1;
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
y <-        c(0, 0, 0, 1)

f <- function(w, x, b) w %*% t(x) + b
step <- function(x) ifelse(x < 0, 0, 1)

for (i in 1:10) {
  f_1 <- f(w, x, b)
  w <- w - c(sum(learn * (2 * (pmax(0, f_1) - y) * step(f_1) * x[,1])),
             sum(learn * (2 * (pmax(0, f_1) - y) * step(f_1) * x[,2])))
  b <- b - sum(learn * (2 * (pmax(0, f_1) - y)) * step(f_1))
}
```

Finally, we can verify the results,

```{r}
(f(w, x, b) > 0.5) == as.logical(y)
```
