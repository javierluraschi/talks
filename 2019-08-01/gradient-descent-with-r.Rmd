---
title: "Gradient Descent in Base R"
output: html_notebook
---

Let's start with a perceptron,

$$
f(x) =
\begin{cases}
1 & \sum_{i=1}^m w_i x_i + b > 0\\
0 & \text{otherwise}
\end{cases}
$$

Gradient descent minimizes a function by following the direction of maximum change:

$$
a_{n+1} = a_n - \gamma \nabla F(a_n)
$$

But we use a differentiable perceptron,

$$ max(0,\sum_{i=1}^m w_i x_i + b) $$

Now we need to minimize,

$$
(f(w_1, w_2, b) - y)^2 = (max(0,(w_1 x_1 + w_2 x_2 + b)) - y)^2
$$


And differentiate,

$$
\frac{df(w_1, w_2, b)}{w_1} = 2 * (f(w_1, w_2, b) - y) * \theta(f(w_1, w_2, b)) * x_1 \\
\frac{df(w_1, w_2, b)}{w_2} = 2 * (f(w_1, w_2, b) - y) * \theta(f(w_1, w_2, b)) * x_2 \\
\frac{df(w_1, w_2, b)}{b} = 2 * (f(w_1, w_2, b) - y) * \theta(f(w_1, w_2, b)) 
$$

To train this single-layer single-perceptron,

```{r}
w_1=0.1; w_2=0.2; b=0.3; learn=0.1;
x <- matrix(c(0, 0, 1, 1,
              0, 1, 0, 1), nrow = 4)
y <-        c(0, 0, 0, 1)

f <- function(w_1, w_2, b, x_1, x_2) w_1 * x_1 + w_2 * x_2 + b
step <- function(x) ifelse(x < 0, 0, 1)

for (i in 1:10) {
  f_1 <- f(w_1, w_2, b, x[,1], x[,2])
  w_1 <- w_1 - sum(learn * (2 * (pmax(0, f_1) - y) * step(f_1) * x[,1]))
  w_2 <- w_2 - sum(learn * (2 * (pmax(0, f_1) - y) * step(f_1) * x[,2]))
  b <- b - sum(learn * (2 * (pmax(0, f_1) - y)) * step(f_1))
}
```

Finally, we can verify the results,

```{r}
(f(w_1, w_2, b, x[,1], x[,2]) > 0.5) == as.logical(y)
```
